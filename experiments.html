<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<!--
Design by TEMPLATED
http://templated.co
Released for free under the Creative Commons Attribution License

Name       : PlainDisplay 
Description: A two-column, fixed-width design with dark color scheme.
Version    : 1.0
Released   : 20140309

-->
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<title>Android Malware Insights 2018</title>
<meta name="keywords" content="" />
<meta name="description" content="" />
<link href="http://fonts.googleapis.com/css?family=Varela" rel="stylesheet" />
<link href="default.css" rel="stylesheet" type="text/css" media="all" />
<link href="fonts.css" rel="stylesheet" type="text/css" media="all" />

<!--[if IE 6]><link href="default_ie6.css" rel="stylesheet" type="text/css" /><![endif]-->

</head>
<body>
<div id="wrapper">
	<div id="header-wrapper">
	<div id="header" class="container">
		<div id="logo">
			<h1><a href="#"></a></h1>
		</div>
		<div id="menu">
			<ul>
				<li><a href="index.html" accesskey="1" title="">Homepage</a></li>
				<li><a href="#" accesskey="2" title=""></a></li>
				<li><a href="downloads.html" accesskey="3" title="">Downloads</a></li>
				<li class="current_page_item"><a href="experiments.html" accesskey="4" title="">Experiments</a></li>
				<li><a href="information.html" accesskey="5" title="">Information</a></li>
			</ul>
		</div>
	</div>
	</div>
	<div id="banner">
		<div class="container">
			<div class="title">
				<h2>Experiments</h2>
				<span class="byline"></span></div>
			<!--<ul class="actions">
				<li><a href="#" class="button">Etiam posuere</a></li>
			</ul>-->
		</div>
	</div>
	<div id="extra" class="container">
		<div class="title">
			<span class="byline">
				We conducted a series of experiments on three malware datasets to (a) check the trends adopted by the malware authors across
				time, and (b) the difficulty of detecting the malicious apps within such datasets. You can find the results of some of the
				experiments we conducted here (particularly the most intriguing ones). You can find a less organized, note-to-self summary
				of the results to follow in <a href="data/stats.txt">here</a>. Please refer to our paper (cited on the <a href="index.html">
				homepage</a>) for more thoughts about the results of the following experiments.
            </span> 
        </div>
	</div>
	
	<!--  Page content here -->
	<div class="container">
		<h2>Trends in current malware datasets</h2>
		<br/>
		<h3>Dataset Composition</h3>
		<br/>
		<p>In this set of experiments, we used our <a href="https://drive.google.com/open?id=1RLz5HTzp1mPSpyxgbBcmrkXSebk-vJmg">encyclopedia</a> to retrieve different information
		about the malicious apps in the Malgenome, Piggybacking, and AMD datasets. We used such information to infer the following insights
		about those datasets.</p>
		
		<h4>Malgenome</h4>
		<p>Malgenome was gathered by Zhou et al. between 2010 and 2012. For years it has been considered the de facto Android malware
		dataset and has, consequently, been used by different researchers to evaluate their analysis/detection approaches and even
		included in newer datasets such as Drebin and AMD. More details about the gathering process can be found in the 
		<a href="information.html#zhou+2012">Zhou+2012</a> paper. Unfortunately, Malgenome is <b style="color:red;">discontinued</b>. 
		However, it is included	in the Drebin dataset, which is still accessible <a href="https://www.sec.cs.tu-bs.de/~danarp/drebin/">here</a>.</p>
		<table>
			<tr><td>Total apps found</td><td>1234 (out of 1260)</td></tr>
			<tr><td>Detected by X VirusTotal scanners (Average)</td><td>31.43</td></tr>
			<tr><td>Source Marketplace(s)</td><td>Various (<a href="information.html#zhou+2012">Zhou+2012</a>)</td></tr>
			<tr><td>Top 10 Families (as per VirusTotal)</td><td>Droidkungfu (38%), BaseBridge (~25%), Geinimi (~5%), Kmin (~4%), Ddlight (3.8%), 
																Golddream (3.7%), PJapps (3.6%), Lotoor (1.8%), yzhc (1.7%), adrd (1.7%)</td></tr>
			<tr><td>Top 10 Types (as per VirusTotal)</td><td>Trojan (94.6%), Exploit (3.4%), Spyware (1.7%), Syp++Trojan (1 app), 
															FakeEnflict++Trojan (1 app)</td></tr>
			<tr><td>% of scanner consensus on Family name</td><td>No (100%): Not a single app had consensus on family name</td></tr>
			<tr><td>% of scanner consensus on Type</td><td>No (100%): Not a single app had consesnsus on type</td></tr>
		</table>
		<h4>Piggybacking</h4>
		<p>The Piggybacking dataset was gathered by Li et al. roughly between 2014 and 2017. The dataset is part of the
		<a href="https://androzoo.uni.lu/access">AndroZoo</a> project, and focuses on repackaged/piggybacked malware.
		The dataset comprises a list of original apps along with their piggybacked versions. We managed to download 1355 original
		apps and 1399 of their repackaged versions (N.B. one benign app can have multiple fake versions). Li et al. reported the
		trends exhibited by the piggybacked apps in their paper <a href="information.html#li+2017">Li+2017</a>. The following figures
		are gathered from statically analyzing the <b>piggybacked (malicious)</b> apps only.</p>
		<table>
			<tr><td>Total apps found</td><td>1136 (out of 1399)</td></tr>
			<tr><td>Detected by X VirusTotal scanners (Average)</td><td>9</td></tr>
			<tr><td>Source Marketplace(s)</td><td>Anzhi (~64%), AppChina (~12%), AnGeeks (5.1%), 1Mobile (5%), Google Play (4.4%),
												Malgenome (3.1%), Google Play + AppChina (8 apps), Anzhi + AppChina (5 apps),
												Slideme (4 apps), Mi.com + Anzhi (4 apps)</td></tr>
			<tr><td>Top 10 Families (as per VirusTotal)</td><td>Dowgin (24.6%), Kuguo (~22%), Gingermaster (~6%), Adwhirlads (5.6%)
														Ginermaster (5.5%), Admobads (~4%), Adwo (~3%), Youmi (2.2%), Droidkungfu (2.1%), 
														Geinimi (1.7%)</td></tr>
			<tr><td>Top 10 Types (as per VirusTotal)</td><td>Adware (64%), Trojan (25%), Spyware (~2.3%), Riskware (2.2%), Adsware (2.2%),
																 Troj (1.9%), Unclassified (6 apps), Adware++Adware (4 apps), TrojanSMS (2 apps), 
																Spr (2 apps)</td></tr>
			<tr><td>% of scanner consensus on Family name</td><td>35% of apps had concensus on family name</td></tr>
			<tr><td>% of scanner consensus on Type</td><td>45% of apps had consensus on type</td></tr>
		</table>
		<h4>AMD</h4>
		<p>The most recent dataset we could find was gathered by Wei et al. in <a href="information.html#wei+2017">Wei+2017</a>. 
		The dataset comprises 24,000 malicious apps gathered from a multitude of marketplaces and older datasets. We randomly sampled
		the dataset to wind up with 1,250 malicious apps that honor the distribution of malware families within the original dataset 
		as reported by the authors on their <a href="http://amd.arguslab.org/">website</a>. Unfortunately, we could gather information about
		a mere 204 apps from our <i>Euphony</i>-based <a href="https://drive.google.com/open?id=1RLz5HTzp1mPSpyxgbBcmrkXSebk-vJmg">encyclopedia</a>.</p>
		<table>
			<tr><td>Total apps found</td><td>204 (out of 1250)</td></tr>
			<tr><td>Detected by X VirusTotal scanners (Average)</td><td>24.6</td></tr>
			<tr><td>Source Marketplace(s)</td><td>Malgenome (29%), Google Play (27.4%), AppChina (23%), Anzhi (10.7%), AnGeeks (~5%), 
												AppChina + Google Play (4 apps), FreewareLovers (1 app), AppChina + Malgenome (1 app)</td></tr>
			<tr><td>Top 10 Families (as per VirusTotal)</td><td>Droidkungfu (20.5%), Airpush (8.8%), Ginmaster (8.3%), Kyview (6.8%), 
															Dowgin (6.3%), GoldDream (5.8%), Nandrobox (5.4%), Lotoor (5.4%), Youmi (8apps), 
															Kuguo (8 apps)</td></tr>
			<tr><td>Top 10 Types (as per VirusTotal)</td><td>Trojan (42.6%), Adware (34.4%), Exploit (11.2%), Riskware (3.9%), 
															Monitor (3.4%), Spyware (1.9%), Hacktool (2 apps), Fakeupdates++Trojan (1 app), 
															Addisplay (1 app)</td></tr>
			<tr><td>% of scanner consensus on Family name</td><td>Only 6% of apps had consensus on family name</td></tr>
			<tr><td>% of scanner consensus on Type</td><td>Only 7% of apps had consensus on type</td></tr>
		</table>
		<p>A summary of the previously-reported results:</p>
		<table>
		  <tr>
		    <th>Dataset</th>
		    <th>Total Apps</th>
		    <th>Average Detection Rate</th>
		    <th>Source (s)</th>
		    <th>Top Families</th>
		    <th>Top Types</th>
		  </tr>
		  <tr>
		    <td>Malgenome</td>
		    <td>1234</td>
		    <td>31.4</td>
		    <td>Various</td>
		    <td>Droidkungfu (38%), Basebridge (25%), Geinimi (5%)</td>
		    <td>Trojan (94%), Exploit (3%), Spyware (1%)</td>
		  </tr>
		  <tr>
		    <td>Piggybacking (Malicious Apps)</td>
		    <td>1136</td>
		    <td>9</td>
		    <td>Anzhi (64%), AppChina (12%), AnGeeks (5%)</td>
		    <td>Dowgin (24%), Kuguo (22%), Gingermaster (6%)</td>
		    <td>Adware (64%), Trojan (25%), Spyware (2%)</td>
		  </tr>
		  <tr>
		    <td>AMD</td>
		    <td>204 (out of 1250)</td>
		    <td>24.6</td>
		    <td>Malgenome (29%), Google Play (27%), AppChina (23%)</td>
		    <td>Droidkungfu (20%), Airpush (8%), Ginmaster (8%)</td>
		    <td>Trojan (42%), Adware (34%), Exploit (11%)</td>
		  </tr>
		</table>
		<br/>
	
		<h3>Repackaging Trends</h3>
		<br/>
		<p>Technically, it is quite straightforward to download an app, disassemble/decompile it, add some (malicious) code to its 
		original code, re-assemble/compile it, sign it, and upload it to a (different) Android app marketplace. This techniques is
		usually referred to as repackaging/piggybacking. It is considered as an effective distribution method that trick users into
		voluntarily infecting their devices by downloading an installing what appears to be legitimate apps (e.g., a new version of
		Angry Birds). In this experiment, we wanted to measure the percentage of apps in each dataset that adopt such technique.
		To do so, we relied on a compiler fingerprinting tool, <a href="https://github.com/rednaga/APKiD">APKiD</a>, to retrieve the compiler used to compile an app. If the compiler
		is not <i>dex</i> or <i>dexmerge</i>, which are used by the Android SDK, we assume that the "developer" did not have access to
		the app's souce code and had to rely on tools such as <a href="http://ibotpeaches.github.io/Apktool/">Apktool</a> (and the compilers it utilizes such as <i>dexlib</i>)to compile
		the app (probably during repackaging).</p>
		<table>
		  <tr>
		    <th>Dataset</th>
		    <th>dx</th>
		    <th>dexmerge</th>
		    <th>Not Repackaged (dx + dexmerge)</th>
		    <th>Repackaged (dexlib 1.X + 2.X)</th>
		  </tr>
		  <tr>
		    <td>Malgenome</td>
		    <td>52%</td>
		    <td>--</td>
		    <td>52%</td>
		    <td>48%</td>
		  </tr>
		  <tr>
		    <td>Play Store (benign)</td>
		    <td>61%</td>
		    <td>34%</td>
		    <td>95%</td>
		    <td>5%</td>
		  </tr>
		  <tr>
		    <td>Piggybacking (malicious)</td>
		    <td>22%</td>
		    <td>6%</td>
		    <td>28%</td>
		    <td>72%</td>
		  </tr>
		  <tr>
		    <td>Piggybacking (benign)</td>
		    <td>61%</td>
		    <td>22%</td>
		    <td>83%</td>
		    <td>17%</td>
		  </tr>
		  <tr>
		    <td>AMD</td>
		    <td>38%</td>
		    <td>35%</td>
		    <td>73%</td>
		    <td>~27%</td>
		  </tr>
		</table>
		<br/>
		<hr></hr>
		<h2>Detection Experiments</h2>
		<p>In this set of experiments, we wish to gain insights about the difficulty to detect malicious apps in the three datasets we are
		focusing on. Furthermore, we wish to investigate the cross-dataset detection capability. In other words, we wish to study whether 
		a detection method built on top of a dataset can recognize malicious instances from another, possibly newer, dataset. This gives us
		a rough estimation about the lifespan of a dataset, within which it can be used to train effective classifiers.</p>
		<p>Our detection experiments are based on machine learning classifiers trained using the static and dynamic features listed in the
		<a href="information.html">information</a> page. To have a comprehensive view of the performance of different classifiers, we 
		utilize a voting classifier trained using K-Nearest Neighbors with different values of K={10, 25, 50, 100, 250, 500}, random forests
		with different number of trees E={10, 25, 50, 75, 100}, and a Support Vector Machine with the linear kernel.</p>
		<p>We encourage visitors of this website to reproduce our results performed using the feature vectors and Python scripts available
		under our <a href="downloads.html">downloads</a> page.</p>
		<br/>
		<h3>Benchmark Classification</h3>
		<br/>
		<p>Prior to conducting the main experiments in this set, we trained and validated the voting classifier using 10-Fold 
		cross-validation on each dataset individually. The table below summarizes the performance of our ensemble classifier on 
		different datasets using static and dynamic features.</p>
		<table>
		  <tr>
		    <th rowspan="2">Dataset</th>
		    <th colspan="2">Accuracy</th>
		    <th colspan="2">Recall</th>
		    <th colspan="2">Precision</th>
		    <th colspan="2">Specificity</th>
		    <th colspan="2">F1 Score</th>
		  </tr>
		  <tr>
		    <td>Static</td>
		    <td>Dynamic</td>
		    <td>Static</td>
		    <td>Dynamic</td>
		    <td>Static</td>
		    <td>Dynamic</td>
		    <td>Static</td>
		    <td>Dynamic</td>
		    <td>Static</td>
		    <td>Dynamic</td>
		  </tr>
		  <tr>
		    <td>Malgenome+GPlay</td>
		    <td>0.98</td>
		    <td>0.94</td>
		    <td>0.97</td>
		    <td>0.94</td>
		    <td>0.99</td>
		    <td>0.74</td>
		    <td>0.99</td>
		    <td>0.94</td>
		    <td>0.98</td>
		    <td>0.83</td>
		  </tr>
		  <tr>
		    <td>Piggybacking</td>
		    <td>0.67</td>
		    <td>0.67</td>
		    <td>0.70</td>
		    <td>0.70</td>
		    <td>0.63</td>
		    <td>0.76</td>
		    <td>0.65</td>
		    <td>0.61</td>
		    <td>0.63</td>
		    <td>0.73</td>
		  </tr>
		  <tr>
		    <td>AMD+GPlay</td>
		    <td>0.94</td>
		    <td>0.87</td>
		    <td>0.92</td>
		    <td>0.87</td>
		    <td>0.96</td>
		    <td>0.86</td>
		    <td>0.96</td>
		    <td>0.87</td>
		    <td>0.94</td>
		    <td>0.86</td>
		  </tr>
		</table>
		<br/>
		
		<h4>Visualization</h4>
		<br/>
		<p>In this set of experiments, we attempt to visually complement the results tabulated above by visualizing the relationship
		between malicious and benign apps in the previous experiment in 2- and 3-dimensional scatter plots. One can notice that there
		is a relationship between the performance of the ensemble classifier on a certain dataset and the distance between the apps within
		the same dataset. In other words, we argue that the t-SNE representation of apps in a lower dimensionality might imply the ease with 
		which the ensemble classifier can separate such apps in a higher dimensionality.</p>
		<p>The following table contains links to HTML 2- and 3-dimensional visualizations of malicious (in red) and benign (in blue) apps
		in the three datasets we experimented on earlier. The visualizations are scatter plots generated using the Python bindings of the
		<a href="https://plot.ly/python/">Plotly</a> graphic library. You can interact with the figures by zooming in/out and rotating
		the camera (only for 3-d figures). Please feel free to download the figures.</p>
		<br/>
		<table>
		  <tr>
		    <th rowspan="2">Dataset</th>
		    <th colspan="2">Static Features</th>
		    <th colspan="2">Dynamic Features</th>
		  </tr>
		  <tr>
		    <td>2-d</td>
		    <td>3-d</td>
		    <td>2-d</td>
		    <td>3-d</td>
		  </tr>
		  <tr>
		    <td>Malgenome+GPlay</td>
		    <td><a href="figures/static_visualization/malgenome+gplay_static_2d.html">view</a></td>
			<td><a href="figures/static_visualization/malgenome+gplay_static_3d.html">view</a></td>		    
			<td><a href="figures/dynamic_visualization/malgenome+gplay_dynamic_2d.html">view</a></td>
		    <td><a href="figures/dynamic_visualization/malgenome+gplay_dynamic_3d.html">view</a></td>
		  </tr>
		  <tr>
		    <td>Piggybacking</td>
		    <td><a href="figures/static_visualization/piggybacking_static_2d.html">view</a></td>
			<td><a href="figures/static_visualization/piggybacking_static_3d.html">view</a></td>		    
			<td><a href="figures/dynamic_visualization/piggybacking_dynamic_2d.html">view</a></td>
		    <td><a href="figures/dynamic_visualization/piggybacking_dynamic_3d.html">view</a></td>
		  </tr>
		  <tr>
		    <td>AMD+GPlay</td>
		    <td><a href="figures/static_visualization/amd+gplay_static_2d.html">view</a></td>
			<td><a href="figures/static_visualization/amd+gplay_static_3d.html">view</a></td>		    
			<td><a href="figures/dynamic_visualization/amd+gplay_dynamic_2d.html">view</a></td>
		    <td><a href="figures/dynamic_visualization/amd+gplay_dynamic_3d.html">view</a></td>
		  </tr>
		</table>
		
		<h3>Most difficult to detect</h3>
		<br/>
		<p>Given the poor performance achieved by the voting classifier on the Piggybacking dataset, we wanted to gain more insights about
		the malware families and types that proved to be difficult to detect by such classifier. The following figures depict the average 
		percentages (across 10-Folds) of malware families and types in the Piggybacking dataset that were misclassified by the 
		aforementioned classifier:</p>
		<h4>Using static features</h4>
		<ul style="margin: 20px">
		<li style="margin:.5em 10% .5em 0"><u>Misclassified malware families</u>: Admobads (23%), Dowgin (~20%), Kuguo (~12%), Wooboo (9.1%), 
											Youmi (8%), UMeng (7.3%), Geinimi (6.2%), Domob (5.1%), Tapjoyads (4.5%), Droidkungfu (4.5%)</li>
		<li style="margin:.5em 10% .5em 0"><u>Misclassified malware types</u>: Adware (61%), Trojan (20.6%), Adsware (7%), Troj (5.5%),
											Unclassified (1.5%), Riskware (1.2%), Spyware (~1%), Adware++Adware (~1%), 
											Exploit (0.9%), Virus (0.8%)</li>
		</ul>
		
		<h4>Using dynamic features</h4>
		<ul style="margin: 20px">
		<li style="margin:.5em 10% .5em 0"><u>Misclassified malware families</u>: Admobads (25%), Kuguo (17%), BoqX (11.7%), Adwo (7.8%), 
											Adwhirlads (7.8%), Youmi (~6%), UMeng (~6%), Gingermaster (~6%), Droidkungfu (~6%), Domob (~6%)
											 </li>
		<li style="margin:.5em 10% .5em 0"><u>Misclassified malware types</u>: Adware (48.2%), Trojan (22.3%), Troj (9.4%), Adsware (7%)
											Spyware (4.7%), Riskware (3.5%), Adware++Adware (2.3%), Unclassified (1.1%), Rootor++Spyware (1.1%)</li>
		</ul>
		
		<h3>Compatibility Experiments</h3>
		<br/>
		<p>In this set of experiments, we trained the voting classifier using one dataset and tested it using another dataset. Due to 
		the differences in the release dates of such datasets, each experiment can give us indications about the forward, lateral, or 
		backward applicability of each dataset. For example, in the first row, the classifier is trained using malicious apps from the 
		Malgenome dataset (released in 2012) and benign apps we gathered in 2017 from the Google Play marketplace. The classifier is 
		then tested using malicious and benign apps from the Piggybacking dataset (released around 2017). Thus, this experiment tests
		 the forward applicability of the Malgenome dataset. In other words, such experiments enable us to answer questions such as 
		 how much the malicious apps have changed over the years, how comprehensive the training dataset is, and how long it takes 
		 for a dataset to be obsolete (i.e., unable to help classifiers recognize novel malware instances).</p>
		<table style="font-size: 12px">
		  <tr>
		    <th rowspan="2">Training Dataset</th>
		    <th rowspan="2">Test Dataset</th>
		    <th colspan="2">Accuracy</th>
		    <th colspan="2">Recall</th>
		    <th colspan="2">Precision</th>
		    <th colspan="2">Specificity</th>
		    <th colspan="2">F1 Score</th>
		  </tr>
		  <tr>
		    <td>Static</td>
		    <td>Dynamic</td>
		    <td>Static</td>
		    <td>Dynamic</td>
		    <td>Static</td>
		    <td>Dynamic</td>
		    <td>Static</td>
		    <td>Dynamic</td>
		    <td>Static</td>
		    <td>Dynamic</td>
		  </tr>
		  <tr>
		    <td>Malgenome+GPlay</td>
		    <td>Piggybacking</td>
		    <td>0.49</td>
		    <td>0.52</td>
		    <td>0.49</td>
		    <td>0.65</td>
		    <td>0.54</td>
		    <td>0.42</td>
		    <td>0.47</td>
		    <td>0.44</td>
		    <td>0.51</td>
		    <td>0.51</td>
		  </tr>
		  <tr>
		    <td>Malgenome+GPlay</td>
		    <td>AMD+GPlay</td>
		    <td>0.90</td>
		    <td>0.79</td>
		    <td>0.96</td>
		    <td>0.93</td>
		    <td>0.83</td>
		    <td>0.60</td>
		    <td>0.86</td>
		    <td>0.73</td>
		    <td>0.90</td>
		    <td>0.73</td>
		  </tr>
		  <tr>
		    <td>AMD+GPlay</td>
		    <td>Piggybacking</td>
		    <td>0.50</td>
		    <td>0.59</td>
		    <td>0.50</td>
		    <td>0.63</td>
		    <td>0.75</td>
		    <td>0.73</td>
		    <td>0.48</td>
		    <td>0.78</td>
		    <td>0.60</td>
		    <td>0.69</td>
		  </tr>
		  <tr>
		    <td>Piggybacking</td>
		    <td>AMD+GPlay</td>
		    <td>0.47</td>
		    <td>0.63</td>
		    <td>0.47</td>
		    <td>0.57</td>
		    <td>0.48</td>
		    <td>0.86</td>
		    <td>0.48</td>
		    <td>0.78</td>
		    <td>0.47</td>
		    <td>0.69</td>
		  </tr>
		  <tr>
		    <td>AMD+GPlay</td>
		    <td>Malgenome+GPlay</td>
		    <td>0.97</td>
		    <td>0.93</td>
		    <td>0.95</td>
		    <td>0.92</td>
		    <td>0.99</td>
		    <td>0.93</td>
		    <td>0.99</td>
		    <td>0.94</td>
		    <td>0.97</td>
		    <td>0.92</td>
		  </tr>
		  <tr>
		    <td>Piggybacking</td>
		    <td>Malgenome+GPlay</td>
		    <td>0.51</td>
		    <td>0.63</td>
		    <td>0.51</td>
		    <td>0.55</td>
		    <td>0.34</td>
		    <td>0.94</td>
		    <td>0.51</td>
		    <td>0.89</td>
		    <td>0.40</td>
		    <td>0.70</td>
		  </tr>
		</table>
		<br/>
		
		<h3>Adversarial Experiments</h3>
		<br/>
		<p>This set of experiments builds on the observations and conclusions drawn from the experiments conducted in previous sections. 
		Despite the simplicity of the static and dynamic features we utilize in this paper, we were able to use them to train voting 
		classifiers that perform well on the Malgenome and AMD datasets. Nonetheless, our classifiers severely underperformed on the 
		Piggybacking dataset. In this context, we conducted two sets of experiments that attempt to answer two questions. Firstly, is 
		repackaging the reason behind the difficulty to detect apps in the Piggybacking dataset? Secondly, if so, how can malware 
		authors leverage this fact to write evasive malware?</p>
		<p>To answer these questions, we separated the malicious and benign subsets of the Piggybacking datasets and included them
		in the training and test datasets in a manner similar to that of the compatibility experiments. We refer to the malicious and benign 
		subsets of the Piggybacking dataset as Piggybacked and Original, respectively.</p>
		<table>
		  <tr>
		    <th rowspan="2">#</th>
		    <th rowspan="2">Training Dataset</th>
		    <th rowspan="2">Test Dataset</th>
		    <th colspan="2">Accuracy</th>
		  </tr>
		  <tr>
		    <td>Static</td>
		    <td>Dynamic</td>
		  </tr>
		  <tr>
		    <td>1</td>
		    <td>AMD+GPlay</td>
		    <td>Piggybacked</td>
		    <td>0.81</td>
		    <td>0.72</td>
		  </tr>
		  <tr>
		    <td>2</td>
		    <td>AMD+GPlay</td>
		    <td>Original</td>
		    <td>0.20</td>
		    <td>0.38</td>
		  </tr>
		  <tr>
		    <td>3</td>
		    <td>AMD+Original</td>
		    <td>Piggybacked</td>
		    <td>0.17</td>
		    <td>0.50</td>
		  </tr>
		  <tr>
		    <td>4</td>
		    <td>AMD+Original</td>
		    <td>Original</td>
		    <td>0.98</td>
		    <td>0.94</td>
		  </tr>
		  <tr>
		    <td>5</td>
		    <td>AMD+Malgenome+GPlay</td>
		    <td>Piggybacked</td>
		    <td>0.81</td>
		    <td>0.79</td>
		  </tr>
		  <tr>
		    <td>6</td>
		    <td>AMD+Malgenome+GPlay</td>
		    <td>Original</td>
		    <td>0.20</td>
		    <td>0.30</td>
		  </tr>
		  <tr>
		    <td>7</td>
		    <td>AMD+Original+GPlay</td>
		    <td>Piggybacked</td>
		    <td>0.19</td>
		    <td>0.34</td>
		  </tr>
		  <tr>
		    <td>8</td>
		    <td>AMD+Original+GPlay</td>
		    <td>Original</td>
		    <td>0.98</td>
		    <td>0.98</td>
		  </tr>
		  <tr>
		    <td>9</td>
		    <td>AMD+Malgenome+Original+GPlay</td>
		    <td>Piggybacked</td>
		    <td>0.30</td>
		    <td>0.43</td>
		  </tr>
		  <tr>
		    <td>10</td>
		    <td>AMD+Malgenome+Original+GPlay</td>
		    <td>Original</td>
		    <td>0.91</td>
		    <td>0.92</td>
		  </tr>
		</table>
		<br/>
	
	</div>
	
	
	<div id="copyright" class="container">
	<p>Photos by <a href="http://fotogrph.com/">Fotogrph</a> | Design by <a href="http://templated.co" rel="nofollow">TEMPLATED</a>.</p>
</div> 
</body>
</html>
